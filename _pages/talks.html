---
layout: single
title: "Projects"
permalink: /talks/
author_profile: true
---

<section aria-labelledby="mpc-title">
  <h2 id="mpc-title">
    <a href="https://amrmarey15.github.io/files/MPC_RobotArm.pdf" target="_blank" rel="noopener noreferrer">
      Exploring Nonlinear Model Predictive Control in Two-Link Robot Arm Motion Planning
    </a>
  </h2>

  <h3>Goal</h3>
  <p>
    Investigates nonlinear Model Predictive Control (MPC) for motion planning in a two-link robot arm, aiming to improve trajectory efficiency under constraints.
  </p>

  <h3>Approach</h3>
  <ul>
    <li>Derived robot dynamics using Lagrangian formulation and state-space models.</li>
    <li>Formulated optimal control problem with state/input constraints.</li>
    <li>Discretized system for discrete-time MPC.</li>
    <li>
      Explored two MPC strategies:
      <ul>
        <li><strong>Long prediction horizon (Approach A):</strong> Approximates infinite-horizon MPC.</li>
        <li><strong>Terminal state constraints (Approach B):</strong> Shorter horizon but requires terminal region condition.</li>
      </ul>
    </li>
  </ul>

  <h3>Results</h3>
  <ul>
    <li>Both approaches achieved the target configuration while avoiding obstacles.</li>
    <li>Longer horizons (Approach A, N = 30) improved accuracy and reduced overshoot but required higher computation.</li>
    <li>Terminal constraints (Approach B, N = 15, &epsilon; = 0.1) allowed smaller horizons with lower computation, but feasibility issues arose with small &epsilon;.</li>
    <li>Failure cases showed instability or infeasibility with too small horizons or overly strict terminal constraints.</li>
  </ul>

  <h3>Discussion</h3>
  <ul>
    <li>Trade-off between horizon length and computational complexity.</li>
    <li>Cost function weights (Q and R) significantly influenced trajectory smoothness vs. energy use.</li>
    <li>A good initial guess for the optimizer was necessary due to non-convex dynamics.</li>
  </ul>

  <h3>Future Work</h3>
  <ul>
    <li>Apply to real robots or more realistic models.</li>
    <li>Explore adaptive horizons or minimum-time MPC.</li>
    <li>Extend to dynamic/moving obstacles.</li>
    <li>Study robustness under non-ideal effects (e.g., friction).</li>
  </ul>
</section>



<section aria-labelledby="ppo-title">
  <h2 id="ppo-title">
    <a href="https://amrmarey15.github.io/files/PPO.pdf" target="_blank" rel="noopener noreferrer">
      Project II: PPO with Compressed RGB Inputs for Robotic Grasping Problem
    </a>
  </h2>

  <h3>Goal</h3>
  <p>
    Explore whether variational auto-encoders (VAEs) and &beta;-VAEs can compress high-dimensional RGB inputs into useful
    latent representations for a PPO agent in a robotic grasping task.
  </p>

  <h3>Method</h3>
  <ul>
    <li>Collected ~111k RGB images from a simulated Kuka grasping environment.</li>
    <li>Trained multiple VAEs/&beta;-VAEs with varying latent dimensions (1–1000) and &beta; values (0.1, 1, 10).</li>
    <li>Used compressed latent vectors as input to PPO agents and compared against a baseline PPO trained on raw images.</li>
  </ul>

  <h3>Results</h3>
  <ul>
    <li><strong>Baseline PPO:</strong> mean reward &asymp; 0.42.</li>
    <li><strong>Best performance:</strong> latent dimension = 10, &beta; = 0.1 &rarr; mean reward &asymp; 0.54.</li>
    <li>Smaller latent spaces (dim = 2) were stable across &beta; values.</li>
    <li>Larger latent dimensions degraded with higher &beta;.</li>
    <li>&beta;-VAEs sometimes outperformed standard VAEs.</li>
  </ul>

  <figure>
    <figcaption>Score summary</figcaption>
    <table>
      <thead>
        <tr>
          <th>Setup</th>
          <th>Latent Dim</th>
          <th>&beta;</th>
          <th>Mean Reward</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>Baseline PPO (raw RGB)</td>
          <td>—</td>
          <td>—</td>
          <td>&asymp; 0.42</td>
        </tr>
        <tr>
          <td>Best VAE/&beta;-VAE</td>
          <td>10</td>
          <td>0.1</td>
          <td>&asymp; 0.54</td>
        </tr>
      </tbody>
    </table>
  </figure>

  <h3>Discussion</h3>
  <ul>
    <li>VAEs can improve PPO performance but do not guarantee faster convergence.</li>
    <li>Trade-offs exist between latent size, disentanglement (&beta;), and policy learning stability.</li>
  </ul>

  <h3>Future Work</h3>
  <p>
    Test with other RL algorithms, more complex tasks, real robots, and deeper analysis of latent representations.
  </p>
</section>

