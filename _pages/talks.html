---
layout: single
title: "Projects"
permalink: /talks/
author_profile: true
---

<section aria-labelledby="mpc-title">
  <h2 id="mpc-title">
    <a href="https://amrmarey15.github.io/files/MPC_RobotArm.pdf" target="_blank" rel="noopener noreferrer">
      Exploring Nonlinear Model Predictive Control in Two-Link Robot Arm Motion Planning
    </a>
  </h2>

  <h3>Goal</h3>
  <p>
    Investigates nonlinear Model Predictive Control (MPC) for motion planning in a two-link robot arm, aiming to improve trajectory efficiency under constraints.
  </p>

  <h3>Approach</h3>
  <ul>
    <li>Derived robot dynamics using Lagrangian formulation and state-space models.</li>
    <li>Formulated optimal control problem with state/input constraints.</li>
    <li>Discretized system for discrete-time MPC.</li>
    <li>
      Explored two MPC strategies:
      <ul>
        <li><strong>Long prediction horizon (Approach A):</strong> Approximates infinite-horizon MPC.</li>
        <li><strong>Terminal state constraints (Approach B):</strong> Shorter horizon but requires terminal region condition.</li>
      </ul>
    </li>
  </ul>

  <h3>Results</h3>
  <ul>
    <li>Both approaches achieved the target configuration while avoiding obstacles.</li>
    <li>Longer horizons (Approach A, N = 30) improved accuracy and reduced overshoot but required higher computation.</li>
    <li>Terminal constraints (Approach B, N = 15, &epsilon; = 0.1) allowed smaller horizons with lower computation, but feasibility issues arose with small &epsilon;.</li>
    <li>Failure cases showed instability or infeasibility with too small horizons or overly strict terminal constraints.</li>
  </ul>

  <h3>Discussion</h3>
  <ul>
    <li>Trade-off between horizon length and computational complexity.</li>
    <li>Cost function weights (Q and R) significantly influenced trajectory smoothness vs. energy use.</li>
    <li>A good initial guess for the optimizer was necessary due to non-convex dynamics.</li>
  </ul>

  <h3>Future Work</h3>
  <ul>
    <li>Apply to real robots or more realistic models.</li>
    <li>Explore adaptive horizons or minimum-time MPC.</li>
    <li>Extend to dynamic/moving obstacles.</li>
    <li>Study robustness under non-ideal effects (e.g., friction).</li>
  </ul>
</section>



<section aria-labelledby="ppo-title">
  <h2 id="ppo-title">
    <a href="https://amrmarey15.github.io/files/PPO.pdf" target="_blank" rel="noopener noreferrer">
      Project II: PPO with Compressed RGB Inputs for Robotic Grasping Problem
    </a>
  </h2>

  <h3>Goal</h3>
  <p>
    Explore whether variational auto-encoders (VAEs) and &beta;-VAEs can compress high-dimensional RGB inputs into useful
    latent representations for a PPO agent in a robotic grasping task.
  </p>

  <h3>Method</h3>
  <ul>
    <li>Collected ~111k RGB images from a simulated Kuka grasping environment.</li>
    <li>Trained multiple VAEs/&beta;-VAEs with varying latent dimensions (1–1000) and &beta; values (0.1, 1, 10).</li>
    <li>Used compressed latent vectors as input to PPO agents and compared against a baseline PPO trained on raw images.</li>
  </ul>

  <h3>Results</h3>
  <ul>
    <li><strong>Baseline PPO:</strong> mean reward &asymp; 0.42.</li>
    <li><strong>Best performance:</strong> latent dimension = 10, &beta; = 0.1 &rarr; mean reward &asymp; 0.54.</li>
    <li>Smaller latent spaces (dim = 2) were stable across &beta; values.</li>
    <li>Larger latent dimensions degraded with higher &beta;.</li>
    <li>&beta;-VAEs sometimes outperformed standard VAEs.</li>
  </ul>

  <figure>
    <figcaption>Score summary</figcaption>
    <table>
      <thead>
        <tr>
          <th>Setup</th>
          <th>Latent Dim</th>
          <th>&beta;</th>
          <th>Mean Reward</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>Baseline PPO (raw RGB)</td>
          <td>—</td>
          <td>—</td>
          <td>&asymp; 0.42</td>
        </tr>
        <tr>
          <td>Best VAE/&beta;-VAE</td>
          <td>10</td>
          <td>0.1</td>
          <td>&asymp; 0.54</td>
        </tr>
      </tbody>
    </table>
  </figure>

  <h3>Discussion</h3>
  <ul>
    <li>VAEs can improve PPO performance but do not guarantee faster convergence.</li>
    <li>Trade-offs exist between latent size, disentanglement (&beta;), and policy learning stability.</li>
  </ul>

  <h3>Future Work</h3>
  <p>
    Test with other RL algorithms, more complex tasks, real robots, and deeper analysis of latent representations.
  </p>
</section>


<section aria-labelledby="avd-title">
  <h2 id="avd-title">
    <a href="https://amrmarey15.github.io/files/AVD.pdf" target="_blank" rel="noopener noreferrer">
      Project III: Automatic Violence Detection through Skeletal-Estimation Based Deep Learning and Video Data Augmentation
    </a>
  </h2>

  <h3>Goal</h3>
  <p>
    Improve automatic violence detection (AVD) models’ robustness to noise distortions in surveillance video streams.
  </p>

  <h3>Approach</h3>
  <ul>
    <li>Baseline model uses a two-stream ConvLSTM (RGB + change detection) with OpenPose for skeletal estimation.</li>
    <li>
      Proposed data augmentation (DA) strategy applies horizontal flipping, brightness scaling, and noise injection
      (Gaussian + salt &amp; pepper) to training videos.
    </li>
    <li>Training used the RWF-2000 surveillance dataset with added DA videos (5,600 extra samples).</li>
  </ul>

  <h3>Results</h3>
  <ul>
    <li><strong>Accuracy (clean data):</strong> Baseline 87.00%; DA model 87.25%.</li>
    <li><strong>Gaussian noise (σ &gt; 4):</strong> DA model achieved ~10.35% higher accuracy on average than baseline.</li>
    <li><strong>Salt &amp; pepper noise:</strong> DA model averaged ~9.83% higher accuracy.</li>
    <li><strong>Conclusion:</strong> Data augmentation significantly improves noise robustness of AVD models without altering architecture.</li>
  </ul>

  <h3>Future Work</h3>
  <ul>
    <li>Explore GAN-based augmentation.</li>
    <li>End-to-end OpenPose training.</li>
    <li>Application to more challenging real-world settings.</li>
  </ul>
</section>

<section aria-labelledby="boost-title">
  <h2 id="boost-title">
    <a href="https://amrmarey15.github.io/files/boost.pdf" target="_blank" rel="noopener noreferrer">
      Project IV: Exploring the Control of DC–DC Boost Converter
    </a>
  </h2>

  <h3>Goal</h3>
  <p>
    Design and compare different control strategies (LQR, linear MPC, nonlinear MPC, and Pontryagin’s Minimum Principle)
    for regulating the output voltage of a DC–DC boost converter (from 3.3&nbsp;V to 5&nbsp;V) while respecting input/state constraints.
  </p>

  <h3>System Modeling</h3>
  <ul>
    <li>Derived state-space model by averaging circuit equations across switch positions.</li>
    <li>Defined control input as the duty cycle deviation, with equilibrium at the origin.</li>
    <li>Selected inductor (7.5&nbsp;mH) and capacitor (12&nbsp;&micro;F) values based on ripple constraints and designed state/input bounds.</li>
  </ul>

  <h3>LQR Control</h3>
  <ul>
    <li>Linearized the model around the equilibrium and verified controllability and observability.</li>
    <li>Computed the Riccati equation and derived optimal gain <code>K = [0.4459, 0.6564]</code>, achieving stable convergence for both linear and nonlinear systems.</li>
  </ul>

  <h3>Linear MPC</h3>
  <ul>
    <li><strong>Approach 1:</strong> Used a long prediction horizon (optimal <em>N</em> &asymp; 16). Larger horizons didn’t improve performance significantly.</li>
    <li><strong>Approach 2:</strong> Added terminal state constraints, yielding faster convergence and less overshoot compared to Approach&nbsp;1, without needing longer horizons.</li>
  </ul>

  <h3>Nonlinear MPC</h3>
  <ul>
    <li>Using the original nonlinear model with a short horizon (<em>N</em> = 2) outperformed linear MPC, eliminating overshoot and reaching steady state faster.</li>
  </ul>

  <h3>Pontryagin’s Minimum Principle</h3>
  <ul>
    <li>Solved the two-point boundary value problem using collocation to compute optimal state, costate, and control trajectories over a finite horizon.</li>
  </ul>

  <h3>Key Insight</h3>
  <p>
    Nonlinear MPC with short horizons achieved the best performance among all methods; linear MPC with terminal constraints was also effective.
    Parameter tuning of <em>Q</em> could further improve transient response.
  </p>
</section>
